# LSM 树 / 日志结构合并树
LSM 树（Log-Structured Merge Tree）存储引擎介绍  
日志结构的合并树（LSM-tree）是一种基于硬盘的数据结构，与 B-tree 相比，能显著地减少硬盘磁盘臂的开销，并能在较长的时间提供对文件的高速插入（删除），即与 B-tree 相比写性能优化。然而 LSM-tree 在某些情况下，特别是在查询需要快速响应时性能不佳。通常 LSM-tree 适用于索引插入比检索更频繁的应用系统。Bigtable 在提供 Tablet 服务时，使用 GFS 来存储日志和 SSTable，而 GFS 的设计初衷就是希望通过添加新数据的方式而不是通过重写旧数据的方式来修改文件。而 LSM-tree 通过滚动合并和多页块的方法推迟和批量进行索引更新，充分利用内存来存储近期或常用数据以降低查找代价，利用硬盘来存储不常用数据以减少存储代价。  
  
LSM 树 (Log-Structured-Merge-Tree) 的名字往往会给初识者一个错误的印象，事实上，LSM 树并不像 B+ 树、红黑树一样是一颗严格的树状数据结构，它其实是一种存储结构，目前 HBase, LevelDB, RocksDB 这些 NoSQL 存储都是采用的 LSM 树。  
  
LSM 树的核心特点是利用顺序写来提高写性能，但因为分层 (此处分层是指的分为内存和文件两部分) 的设计会稍微降低读性能，但是通过牺牲小部分读性能换来高性能写，使得 LSM 树成为非常流行的存储结构。核心思路就是假定内存足够大，因此不需要每次有数据更新就必须将数据写入到磁盘中，而可以先将最新的数据驻留在内存中，等到积累到最后多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾（因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起）。  
  
## LSM 树的核心思想
[The Secret Sauce Behind NoSQL: LSM Tree](https://www.youtube.com/watch?v=I6jB0nM9SKU)  

![](./LSMTree%20Architecture.jpeg)  
如上图所示，LSM 树有以下三个重要组成部分：  
* MemTable - MemTable 是在内存中的数据结构，用于保存最近更新的数据，会按照 Key 有序地组织这些数据，LSM 树对于具体如何组织有序地组织数据并没有明确的数据结构定义，例如 Hbase 使跳跃表来保证内存中 key 的有序。因为数据暂时保存在内存中，内存并不是可靠存储，如果断电会丢失数据，因此通常会通过 WAL (Write-ahead logging，预写式日志) 的方式来保证数据的可靠性。
* Immutable MemTable - 当 MemTable 达到一定大小后，会转化成 Immutable MemTable。Immutable MemTable 是将转 MemTable 变为 SSTable 的一种中间状态。写操作由新的 MemTable 处理，在转存过程中不阻塞数据更新操作。
* SSTable (Sorted String Table) - 排序字符串表，有序键值对集合，是 LSM 树组在磁盘中的数据结构，因为是有序的所以查找与合并都较为高效 - 类似 merge sort。为了加快 SSTable 的读取，可以通过建立 key 的索引以及布隆过滤器来加快 key 的查找。![](./SSTable.png)
  
这里需要关注一个重点，LSM 树 (Log-Structured-Merge-Tree) 正如它的名字一样，LSM 树会将所有的数据插入、修改、删除等操作记录 (注意是操作记录) 保存在内存之中，当此类操作达到一定的数据量后，再批量地顺序写入到磁盘当中。这与 B+树不同，B+树数据的更新会直接在原数据所在处修改对应的值，但是 LSM 树的数据更新是日志式的，当一条数据更新是直接 append 一条更新记录完成的。这样设计的目的就是为了顺序写，不断地将 Immutable MemTable flush 到持久化存储即可，而不用去修改之前的 SSTable 中的 key，保证了顺序写。  
因此当 MemTable 达到一定大小 flush 到持久化存储变成 SSTable 后，在不同的 SSTable 中，可能存在相同 Key 的记录，当然最新的那条记录才是准确的。这样设计的虽然大大提高了写性能，但同时也会带来一些问题：  
1. 冗余存储，对于某个 key，实际上除了最新的那条记录外，其他的记录都是冗余无用的，但是仍然占用了存储空间。因此需要进行 Compact 操作 (合并多个 SSTable) 来清除冗余的记录。
2. 读取时需要从最新的倒着查询，直到找到某个 key 的记录。最坏情况需要查询完所有的 SSTable，这里可以通过前面提到的索引/布隆过滤器来优化查找速度。
  
## LSM 树的 Compact 策略
Compact 操作是十分关键的操作，否则 SSTable 数量会不断膨胀。在 Compact 策略上，主要介绍 2 种基本策略：**size-tiered** 和 **leveled**。  
在介绍这 2 种策略之前，先介绍 3 个比较重要的概念，事实上不同的策略就是围绕这 3 个概念之间做出权衡和取舍：  
1. 读放大：读取数据时实际读取的数据量大于真正的数据量。例如在 LSM 树中需要先在 MemTable 查看当前 key 是否存在，不存在继续从 SSTable 中寻找。
2. 写放大：写入数据时实际写入的数据量大于真正的数据量。例如在 LSM 树中写入时可能触发 Compact 操作，导致实际写入的数据量远大于该 key 的数据量。
3. 空间放大：数据实际占用的磁盘空间比数据的真正大小更多。上面提到的冗余存储，对于一个 key 来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。  

Compact 策略：  
1. size-tiered 策略 - size-tiered 策略保证每层 SSTable 的大小相近，同时限制每一层 SSTable 的数量。如图，每层限制 SSTable 为 N，当每层 SSTable 达到 N 后，则触发 Compact 操作合并这些 SSTable，并将合并后的结果写入到下一层成为一个更大的 SSTable。由此可以看出，当层数达到一定数量时，最底层的单个 SSTable 的大小会变得非常大。并且 size-tiered 策略会导致空间放大比较严重。即使对于同一层的 SSTable，每个 key 的记录是可能存在多份的，只有当该层的 SSTable 执行 compact 操作才会消除这些key的冗余记录。![](./size-tiered%20策略.png)
2. leveled 策略 - leveled 策略也是采用分层的思想，每一层限制总文件的大小（每一层的总大小固定，从上到下逐渐变大）。但是跟 size-tiered 策略不同的是，leveled 会将每一层切分成多个大小相近的 SSTable。这些 SSTable 是这一层是全局有序的，意味着一个 key 在每一层至多只有 1 条记录，不存在冗余记录。之所以可以保证全局有序，是因为合并策略和 size-tiered 不同，流程如下：1) L1的总大小超过 L1 本身大小限制，2) 此时会从 L1 中选择至少一个文件，然后把它跟 L2 有交集的部分进行合并（比如此时 L1 第二 SSTable 的 key 的范围覆盖了 L2 中前三个 SSTable，那么就需要将 L1 中第二个 SSTable 与 L2 中前三个 SSTable 执行 Compact 操作），生成的文件会放在 L2，3) 如果 L2 合并后的结果仍旧超出的阈值大小，需要重复之前的操作，即选至少一个文件然后把它合并到下一层。需要注意的是，多个不相干的合并是可以并发进行的。![](./leveled%20策略.png)
  
leveled 策略相较于 size-tiered 策略来说，每层内 key 是不会重复的，即使是最坏的情况，除开最底层外，其余层都是重复 key，按照相邻层大小比例为 10 来算，冗余占比也很小。因此空间放大问题得到缓解。但是写放大问题会更加突出。举一个最坏场景，如果 LevelN 层某个 SSTable 的 key 的范围跨度非常大，覆盖了 LevelN+1 层所有 key 的范围，那么进行 Compact 时将涉及 LevelN+1 层的全部数据。  
![](./leveled%20策略%20compact.webp)  
  
## 总结
LSM Tree 弄了很多个小的有序结构，比如每 m 个数据，在内存里排序一次，下面 100 个数据，再排序一次......这样依次做下去，就可以获得 N/m 个有序的小的有序结构。  
在查询的时候，因为不知道这个数据到底是在哪里，所以就从最新的一个小的有序结构里做二分查找，找得到就返回，找不到就继续找下一个小有序结构，一直到找到为止。  
很容易可以看出，这样的模式，读取的时间复杂度是 (N/m)*log2N。读取效率是会下降的。  
这就是最本来意义上的 LSM Tree 的思路。那么这样做，性能还是比较慢的，于是需要再做些事情来提升，怎么做才好呢？  

**LSM Tree 优化方式：**  
* Bloom Filter：就是个带随即概率的 bitmap，可以快速的告知，某一个小的有序结构里有没有指定的那个数据的。于是就可以不用二分查找，而只需简单的计算几次就能知道数据是否在某个小集合里。效率得到了提升，但付出的是空间代价。如果没有 Bloom Filter，则在 LSM 树查找一个本不存在的 key 时，会需要一直往下查到最底层才发现没有，会造成降效，所以在查找读取前可以先用 Bloom Filter 判定是否有该 key，如果没有就直接返回，虽然仍有一些极端情况可能 Bloom Filter 不能正确判定，但总体能很好地改善这类情况。
* Compact：小树合并为大树，因为小树他性能有问题，所以要有个进程不断地将小树合并到大树上，这样大部分的老数据查询也可以直接使用 log2N 的方式找到，不需要再进行 (N/m)*log2n 的查询

通过以上的分析，应该知道 LSM 树的由来了，LSM 树的设计思想非常朴素：将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘，不过读取的时候稍微麻烦，需要合并磁盘中历史数据和内存中最近修改操作，所以写入性能大大提升，读取时可能需要先看是否命中内存，否则需要访问较多的磁盘文件。极端的说，基于 LSM 树实现的 HBase 的写性能比 MySQL 高了一个数量级，读性能低了一个数量级。  

LSM 树原理把一棵大树拆分成 N 棵小树，它首先写入内存中，随着小树越来越大，内存中的小树会 flush 到磁盘中，磁盘中的树定期可以做 merge 操作，合并成一棵大树，以优化读性能。  

## 其他
除了关系数据库的 B+ 树和 NoSQL 的 LSM 树，还有一种是哈希存储引擎：是哈希表的持久化实现，支持增、删、改以及随机读取操作，但不支持顺序扫描，对应的存储系统为 key-value 存储系统。对于 key-value 的插入以及查询，哈希表的复杂度都是 O(1)，明显比树的操作 O(n) 快，如果不需要有序的遍历数据，哈希表就是最适合的。  

## 参考
* https://zhuanlan.zhihu.com/p/181498475
* https://www.modb.pro/db/379790



# Cassandra
这里以 Cassandra 为例子更详细地描述 LSM 树  

写入：  
![](./Cassandra%20Write.png)  
传统的数据库的写入（包括 INSERT、UPDATE、Delete），通常是一个读后写的过程。而 Cassandra 的写入，是没有先读这个动作的，这也是它快的根本原因。一旦使用了 IF NOT EXIST 之类的语法，那么它的写入性能也就会要受损。  

读取：  
![](./Cassandra%20Read.png)  
Cassandra 的每次查询，都会把所有重的 KEY 读出来，但是永远会以最新的 Timestamps 为准。这就解决了把所有的修改，都变成写入的问题。但是，这么干有两大显而易见问题。第一，数据会无限的膨胀，吃掉磁盘。第二，数据膨胀会带来查询需要读出的重复数据增加，无限的膨胀则会无限的增加，读取性能就会受损。  

架构（单个节点内）：  
![](./Cassandra%20Architecture.png)  
只要一个数据库不是内存数据库，那它永远都要面对它最大的性能瓶颈，磁盘 IO。许多概念，比如 Cache、列存储、索引等等，他们优化性能的本质都指向一处，减少磁盘 IO。而对于 SSTable 的读取，其实才是影响性能的关键步骤。  

来看一下，SSTable 到底是什么，它的读取是什么样子的。根据 SSTable 的访问顺序来看，在 3.0 版本中，SSTable 包含以下这么几个文件：  
* Filter.db 这是 SSTable 的 Bloom 过滤器，简单的讲，它告诉你，你要的 Key，在这里有没有。Bloom 过滤器的工作方式是将数据集中的值映射到位数组，并使用散列函数将较大的数据集压缩为摘要字符串。根据定义，摘要使用的内存量比原始数据少得多。它速度快，可能误报，但不会漏。简言之，有可能告诉你有，但是没有。但绝不会告诉没有，却有。这里划一个重点，Cassandra 会维护一个 Bloom filter 的副本在内存里面。所以，这一步不一定会有实际 IO。在书上也提到，如果加大内存，是可以减少 Bloom 过滤器误报的情况。
* Summary.db，这里是索引的抽样，用来加速读取的。
* Index.db，提供 Data.db 里的行列偏移量。
* CompressionInfo.db 提供有关 Data.db 文件压缩的元数据。这里值得关注的是，它用了 Compression 这个词，猜测，如果 Data.db 里面采用了压缩算法，比如说字典压缩之类的，那么这个文件里面应该就会存储字典数据，或者类似的 Compress 相关的元素据。这也就是为什么这个文件，在访问流程中是不可绕过的。因为一旦 Data.db 的数据进行了压缩，那就必须依靠相关的元数据来解压缩数据。从图上可以看出，这个元数据在内存中，相对性能会比较快。
* Data.db 是存储实际数据的文件，是 Cassandra 备份机制保留的唯一文件。它是唯一的真实数据，其他的都是辅助数据。比如索引可以重建，字典可以重建等等。
* Digest.adler32 是 Data.db 校验用的。
* Statistics.db 存储 nodetool tablehistograms 命令使用的有关 SSTable 的统计信息。
* TOC.txt 列出此 SSTable 的文件组件。

其中 1-5 是跟 SSTable 访问数据性能相关的文件。如果 Cache 是 ALL 的情况下，Cassandra 在通常都可以在内存访问之后，直接定位到 SSTable 的具体文件和数据所在偏移量中去。相对于传统数据库，B 树索引层层向下，遇到没有的索引块就要 IO。这个性能应该还是非常可观的。  

## 参考来源
https://www.infoq.cn/article/j0mfq1cntskbk5rbdpvl  



# Lucene
全文搜索的索引引擎，在 Elasticsearch 和 Solr 被使用，它使用类似的方法来存储它的关键词词典。全文索引比键值索引复杂得多，但是基于类似的想法：在搜索查询中，由一个给定的单词，找到提及单词的所有文档（网页，产品描述等）。这也是通过键值结构实现的：其中键是单词（term），值是所有包含该单词的文档的 ID 列表（postings list）。在 Lucene 中，从词语到记录列表的这种映射保存在类似于 SSTable 的有序文件中，并根据需要在后台执行合并。  
