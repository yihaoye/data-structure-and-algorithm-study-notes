# Dynamo

## 目标
设计一个高度可用（即可靠）、高度可扩展、完全去中心化的分布式键值存储。

什么是 Dynamo？
Dynamo 是亚马逊为其内部使用而开发的一种高可用键值存储。许多亚马逊服务，如购物车、畅销书列表、销售排名、产品目录等，只需要对数据进行主键访问。一个多表的关系型数据库系统对于这类服务来说是一种过度，同时也会限制可扩展性和可用性。Dynamo 提供了一个灵活的设计，让应用程序选择他们想要的可用性和一致性水平。


## 背景
Dynamo - 不要与 DynamoDB 混淆，后者的设计灵感来自于 Dynamo - 是一个分布式键值存储系统，在大规模的情况下提供 "永远在线"（或高度可用）的体验。(DDIA 第五章：Dynamo 不适用于 Amazon 以外的用户，令人困惑的是，AWS 提供了一个名为 DynamoDB 的托管数据库产品，它使用了完全不同的体系结构：它基于单主复制)  
在 CAP 定理的术语中，Dynamo 属于 AP 系统的范畴（即可用和分区容忍），并且是以牺牲强一致性为代价来设计高可用性和分区容忍。将 Dynamo 设计成一个高可用系统的主要动机是观察到系统的可用性与所服务的客户数量直接相关。因此，主要目标是系统即使不完善，也应该对客户可用，因为它能带来更多的客户满意度。另一方面，不一致的问题可以在后台解决，而且大多数时候客户不会注意到它们。从这个核心原则出发，Dynamo 积极地对可用性进行了优化。

Dynamo 的设计具有很大的影响力，因为它启发了许多NoSQL数据库，如 Cassandra、Riak 和 Voldemort -- 更不用说亚马逊自己的 DynamoDB。


## 设计目标
如上所述，Dynamo 的主要目标是高度可用。下面是其其他设计目标的总结。

可扩展性。该系统应该是高度可扩展的。我们应该能够把一台机器扔进系统中，看到相应的改进。  
分散的。为了避免单点故障和性能瓶颈，不应该有任何中央/领导的过程。  
最终一致的。数据可以被乐观地复制，成为最终一致的。这意味着，与其为确保整个系统的数据正确性而产生写入时间成本（即强一致性），不如在其他时间（例如，在读取过程中）解决不一致的问题。最终的一致性被用来实现高可用性。  

![](./Dynamo%20High%20Level.png)  
分布式键值存储的高层次视图  


## Dynamo 的使用案例
默认情况下，Dynamo 是一个最终一致的数据库。因此，任何不关心强一致性的应用都可以利用 Dynamo。尽管 Dynamo 可以支持强一致性，但它会对性能产生影响。因此，如果强一致性是一个应用程序的要求，那么 Dynamo 可能不是一个好的选择。  

Dynamo 在亚马逊被用来管理那些对可靠性要求非常高的服务，并且需要严格控制可用性、一致性、成本效益和性能之间的权衡。亚马逊的平台有非常多样化的应用，有不同的存储要求。许多应用选择 Dynamo 是因为它可以灵活地选择适当的权衡，以最经济的方式实现高可用性和保证性能。  

亚马逊平台上的许多服务**只要求对数据存储的主键访问**。对于这样的服务，使用关系型数据库的常见模式会导致效率低下，并限制可扩展性和可用性。Dynamo 提供了一个简单的只有主键的接口，以满足这些应用程序的要求。  


## 系统应用程序接口
Dynamo 客户端使用 put() 和 get() 操作来写入和读取对应于指定键的数据。这个键唯一地标识了一个对象。  
* get(key) - get 操作找到与给定键相关的对象所在的节点，并返回一个单一的对象或一个具有冲突版本的对象列表以及一个上下文。上下文包含了关于对象的编码元数据，这些元数据对调用者没有意义，包括对象的版本等信息（下面会有更多的介绍）。
* put(key, context, object) - put 操作找到与给定键相关的对象应该被存储的节点，并将给定的对象写到磁盘上。上下文是一个值，通过 get 操作返回，然后通过 put 操作送回。上下文总是与对象一起存储，并像一个 cookie 一样用来验证在投放请求中提供的对象的有效性。

Dynamo 将对象和键都视为一个任意的字节数组（通常小于 1 MB）。它在键上应用 MD5 散列算法，生成一个 128 位的标识符，用来确定负责提供键的存储节点。  

<br />
<br />

在高层次上，Dynamo 是一个分布式哈希表（DHT），它在集群中进行复制，以实现高可用性和容错。  

## 介绍 Dynamo 的架构
Dynamo 的架构可以概括为以下几点（将在下面的课程中详细讨论所有这些概念）。  

### 数据分布
Dynamo 使用一致哈希法在节点间分配数据。一致性散列也使得从 Dynamo 集群中添加或删除节点变得容易。  

### 数据复制和一致性
数据是乐观地复制的，即 Dynamo 提供最终的一致性。  

### 处理临时故障
为了处理临时故障，Dynamo 将数据复制到系统中其他节点的模糊法定人数，而不是严格的多数法定人数。  

### 节点间通信和故障检测
Dynamo 的节点使用 gossip 协议来跟踪集群状态。  

### 高可用性
Dynamo 通过使用暗示交接使系统 "永远可写"（或高可用性）。  

### 冲突解决和处理永久性故障
由于没有写入时间保证节点在数值上达成一致，Dynamo 使用其他机制来解决潜在冲突。  

使用矢量时钟来跟踪值的历史，并在读取时调和不同的历史。  
在后台，dynamo 使用像 Merkle 树这样的反熵机制来处理永久性故障。  
后面逐一讨论这些概念。  


## 什么是数据分区？
将数据分布在一组节点上的行为被称为数据分区。当试图分配数据时，有两个挑战。
1. 如何知道某个特定的数据将被存储在哪个节点上？
2. 当添加或删除节点时，如何知道哪些数据将从现有的节点转移到新的节点上？此外，当节点加入或离开时，怎样才能将数据移动降到最低？

一个初级的方法将是使用一个合适的哈希函数，将数据键映射到一个数字。然后，通过对这个数字和服务器的总数应用模数来找到服务器。比如说  
![](./Data%20partitioning%20through%20simple%20hashing.png)    
通过简单的散列法进行数据分区  

上图中描述的方案解决了寻找存储/检索数据的服务器的问题。但是，当增加或删除一个服务器时，必须重新映射所有的键，并根据新的服务器数量移动数据，这将是一个完全混乱的过程  

Dynamo 使用一致性散列来解决这些问题。一致性散列算法帮助 Dynamo 将行映射到物理节点上，也确保在增加或删除服务器时，只有一小部分键会移动。  

### 一致性散列：Dynamo 的数据分布
一致性散列将集群管理的数据表示为一个环。环中的每个节点都被分配一个数据范围。Dynamo 使用一致散列算法来确定什么行被存储到什么节点。下面是一致散列环的一个例子。  
![](./Consistent%20Hashing%20ring.png)  
一致性散列环  

通过一致性散列，环被划分为更小的预定义范围。每个节点被分配到这些范围中的一个。在 Dynamo 的术语中，范围的开始被称为 token。这意味着每个节点将被分配一个令牌 token。分配给每个节点的范围是按以下方式计算的。  

范围开始： token 值  
范围结束： 下一个 token 值 - 1  

下面是上图中描述的四个节点的令牌和数据范围。  
![](./Servers%20and%20token.png)  


每当 Dynamo 为 put() 或 get() 请求提供服务时，它执行的第一个步骤是对键应用 MD5 散列算法。这个散列算法的输出决定了数据位于哪个范围内，因此，数据将被存储在哪个节点上。正如在上面看到的，Dynamo 的每个节点都应该存储一个固定范围的数据。因此，从数据键生成的哈希值得知数据将被存储在哪个节点上。下面是一个例子，显示了数据如何在一致哈希环上分布。  
![](./Distributing%20data%20on%20the%20consistent%20hashing%20ring.png)  
在一致散列环上分布数据  

当一个节点被添加或从环中移除时，上面描述的一致散列方案非常有效；因为在这些情况下，只有下一个节点会受到影响。例如，当一个节点被移除时，下一个节点就会对存储在出站节点上的所有键负责。然而，这种方案会导致数据和负载分布不均匀。Dynamo 在虚拟节点的帮助下解决了这些问题。  

### 虚拟节点
在任何分布式系统中添加和删除节点是很常见的。现有的节点可能会宕机，可能需要退役。同样地，新的节点可能会被添加到现有的集群中，以满足不断增长的需求。Dynamo 通过使用虚拟节点（或 Vnodes）有效地处理这些情况。  

正如在上面看到的，基本的一致性哈希算法为每个物理节点分配了一个令牌 token（或一个连续的哈希范围）。这是一种静态的范围划分，需要根据给定的节点数量来计算令牌。这种方案使得添加或替换一个节点成为一个低效的操作，因为在这种情况下，希望重新平衡并将数据分配给所有其他节点，会导致移动大量数据。下面是一些与手动和固定划分范围相关的潜在问题。  
* 增加或删除节点。增加或删除节点将导致重新计算令牌，对一个大型集群来说，会造成很大的管理开销。
* 热点。由于每个节点被分配一个大范围，如果数据分布不均匀，一些节点会成为热点。
* 节点重建。由于每个节点的数据被复制在固定数量的节点上（后面会讨论），当需要重建一个节点时，只有它的复制节点可以提供数据。这给复制节点带来了很大的压力，并可能导致服务下降。

为了处理这些问题，Dynamo 引入了一种新的方案给物理节点分配令牌 token：与其给一个节点分配一个令牌，不如将哈希范围划分为多个较小的范围，每个物理节点被分配到这些较小范围中的多个。这些子范围中的每一个都被称为一个 Vnode。有了 Vnodes，而不是一个节点只负责一个令牌，而是负责许多令牌（或子范围）。  
![](./Comparing%20Consistent%20Hashing%20ring%20with%20and%20without%20Vnodes.png)  
比较有 Vnodes 和无 Vnodes 的一致性哈希环  

实际上，Vnodes 是随机分布在集群中的，通常是不连续的，所以没有两个相邻的 Vnodes 被分配到同一个物理节点。此外，节点确实携带着其他节点的副本以实现容错。另外，由于集群中可能存在异质机器，一些服务器可能比其他服务器拥有更多的 Vnodes。下图显示了物理节点 A、B、C、D、E 是如何使用一致哈希环的 Vnodes 的。每个物理节点被分配一组 Vnodes，每个 Vnode 被复制一次。  
![](./Mapping%20Vnodes%20to%20physical%20nodes%20on%20a%20Consistent%20Hashing%20ring.png)  
将 Vnodes 映射到一致哈希环的物理节点上  

### Vnodes 的优势
Vnodes 有以下优点。  
1. Vnodes 通过将哈希范围划分为较小的子范围，帮助将负载更均匀地分布在集群上的物理节点上。这加快了添加或删除节点后的再平衡过程。当一个新的节点被添加时，它从现有的节点中接收许多 Vnodes 以保持集群的平衡。同样，当一个节点需要重建时，不是从固定数量的副本中获取数据，而是许多节点参与重建过程。
2. Vnodes 使得维护一个包含异构机器的集群变得更加容易。这意味着，有了 Vnodes，可以给强大的服务器分配较多的范围，给不太强大的服务器分配较少的范围。
3. 由于 Vnodes 有助于为每个物理节点分配较小的范围，因此出现热点的概率比基本的一致哈希方案要小得多，因为该方案每个节点使用一个大范围。


## 什么是乐观复制？
为了确保高可用性和耐久性，Dynamo 将每个数据项在多 (N) 个节点上复制，其中数值 N 为复制因子，可在 Dynamo 的每个实例中配置。每个键被分配给一个协调者节点（在哈希范围内排在第一位的节点），它首先在本地存储数据，然后将其复制到 N-1 顺时针复制到环上的后续节点。这导致每个节点拥有它和它的第 N 个节点之间的环上的区域第 N 个前辈之间的区域。这种复制是异步进行的（在后台），Dynamo 提供了一个最终一致的模型。这种复制技术被称为乐观复制，这意味着不保证复制在任何时候都是相同的。  
![](./Replication%20in%20consistent%20hashing.png)  
一致性散列中的复制  

Dynamo 的每个节点都作为不同范围数据的副本。由于 Dynamo 存储了 N 分布在不同节点上的数据副本，如果一个节点发生故障，其他副本可以响应对该范围数据的查询。如果客户端无法联系到协调者节点，它将请求发送到持有副本的节点。  


## 偏好列表
负责存储特定键的节点的列表被称为偏好列表。Dynamo 的设计是，系统中的每个节点都可以确定哪些节点应该在这个列表中的任何特定键（后面讨论）。这个列表包含超过 N 节点，以考虑故障和跳过环上的虚拟节点，这样列表中只包含不同的物理节点。  


## 宽松的法定人数 (Sloppy Quorum) 和临时故障处理
按照传统的法定人数方法（[Quorum 算法](./../../../Common%20Algorithm%20and%20Theory/Quorum算法.md)），任何分布式系统在服务器故障或网络分区时都会变得不可用，即使在简单的故障条件下也会降低可用性。为了提高可用性，Dynamo 不执行严格的法定人数要求，而是使用一种叫做 "宽松法定人数" 的方法。通过这种方法，所有的读/写操作都在第一个 N 健康的节点上进行，而这些节点可能并不总是第一个 N 节点，而这些节点不一定是在一致散列环上顺时针移动时遇到的第一 N 个节点。  

考虑下图中给出的 Dynamo 配置的例子，N = 3。在这个例子中，如果服务器 1 在写操作过程中暂时宕机或无法到达，其数据现在将被存储在服务器 4 上。因此，Dynamo 将存储在故障节点（即服务器 1）上的副本转移到一致哈希环中没有该副本的下一个节点（即服务器 4）。这样做是为了避免由短期机器或网络故障引起的不可用性，并保持所需的可用性和耐久性保证。发送给服务器 4 的副本在其元数据中会有一个提示，表明哪个节点是该副本的预期接收者（在这种情况下，服务器 1）。收到提示副本的节点将把它们保存在一个单独的本地数据库中，并定期进行扫描。在检测到服务器 1 已经恢复后，服务器4将尝试将副本传递给服务器 1。一旦传送成功，服务器 4 可以从其本地存储中删除该对象，而不减少系统中的副本总数。  
![](./Sloppy%20quorum.png)  
宽松法定人数  


## Hinted Handoff
上面描述的提高可用性的有趣技巧被称为 Hinted handoff，即当一个节点无法访问时，另一个节点可以代表它接受写请求。然后，写内容被保存在本地缓冲区中，一旦目标节点可重新访问，就发送出去。这使得 Dynamo "永远可写"。因此，即使在只有一个节点在线的极端情况下，写请求仍然会被接受并最终被处理。  

主要的问题是，由于宽松法定人数不是严格的多数，数据可以而且会出现分歧，也就是说，对同一个 Key 的两个并发写入有可能被不重叠的节点集所接受。这意味着系统中可能存在针对同一 Key 的多个冲突值，在读取时可能得到陈旧或冲突的数据。Dynamo 允许这种情况，并使用矢量时钟解决这些冲突。  


## 矢量时钟和冲突数据
Dynamo 如何使用矢量时钟来跟踪数据历史？并在读取时调和不同的历史/版本？

正如上面所述，宽松法定人数意味着系统中可能存在针对同一键的多个冲突值，必须以某种方式加以解决。先了解一下这种情况是如何发生的。  

### 什么是时钟偏移？
在一台机器上，需要知道的是绝对时间或挂钟时间：假设对时间戳为 t1 的键 k 进行了一次写入，然后对时间戳为 t2 的 k 进行了另一次写入。由于 t2 > t1，第二次写入的时间肯定比第一次写入的时间新，因此数据库可以安全地覆盖原始值。  

在一个分布式系统中，这个假设并不成立。因为时钟偏移，即不同的时钟往往以不同的速度运行，所以不能假设节点 a 上的时间 t 发生在节点 b 上的时间 t+1 之前。最实用的帮助时钟同步的技术，如 NTP，仍然不能保证分布式系统中的每个时钟在任何时候都是同步的。因此，如果没有像 GPS 装置和原子钟这样的特殊硬件，仅仅使用挂钟的时间戳是不够的。  

### 什么是矢量时钟？
Dynamo 不采用严格的同步机制，而是使用一种叫做矢量时钟的东西，以捕捉同一对象的不同版本之间的因果关系。矢量时钟实际上是一个（节点, 计数器）对的列表。一个向量时钟与存储在 Dynamo 中的每个对象的每个版本相关联。可以通过检查一个对象的两个版本的向量时钟来确定它们是否在平行分支上或有因果顺序。如果第一个对象的时钟上的计数器小于或等于第二个时钟的所有节点，那么第一个对象是第二个对象的祖先，可以被覆盖。否则，这两个变化被认为是冲突的，需要调和。Dynamo 在读取时解决了这些冲突。通过一个例子来理解这一点。  
1. 服务器 A 接收到向键 k1 的一个写请求，其值为 foo。它给它分配的版本是 `[A:1]`。这个写被复制到服务器 B。
2. 服务器 A 接收到向键 k1 的一个写请求，其值为 bar。它给它分配的版本是 `[A:2]`。这个写也被复制到服务器 B。
3. 一个网络分区发生了。A 和 B 不能互相交谈。
4. 服务器 A 接收到向键 k1 的一个写请求，其值为 baz。它给它分配了一个 `[A:3]` 的版本。它不能把它复制到服务器 B，但它被存储在另一个服务器上的 hinted handoff 缓冲区。
5. 服务器 B 接收到向键 k1 的一个写请求，其值为 bax。它给它分配了一个 `[B:1]` 的版本。它不能把它复制到服务器 A，但它被存储在另一台服务器上的 hinted handoff 缓冲区。
6. 网络恢复了。服务器 A 和 B 可以再次相互交谈。
7. 任何一个服务器都会收到一个对键 k1 的读取请求。它看到相同的键有不同的版本 (`[A:3]`) 和 (`[A:2]`, `[B:1]`)，但它不知道哪一个是较新的。它返回这两个版本，并告诉客户端找出版本并将较新的版本写回系统中。

![](./Conflict%20Resolution%20Using%20Vector%20Clocks.png)  

### 使用矢量时钟解决冲突
正如在上面的例子中所看到的，大多数时候，新版本会取代以前的版本，系统本身可以确定正确的版本（例如，`[A:2]` 比 `[A:1]` 新）。然而，版本分支可能发生在故障与并发更新相结合的情况下，导致一个对象的版本冲突。在这些情况下，系统无法调和同一对象的多个版本，客户端必须执行调和，将数据演化的多个分支折叠成一个（这个过程称为语义调和）。折叠操作的一个典型例子是 "合并" 一个客户的购物车的不同版本。使用这种调和机制，一个添加操作（即向购物车添加一个项目）永远不会丢失。然而，被删除的项目可以重新出现。  

解决冲突类似于 Git 的工作方式。如果 Git 可以将不同的版本合并成一个，那么合并就会自动完成。如果不能，客户（即开发者）必须手动协调冲突。  

当向量钟增长过大时，Dynamo 会截断向量钟（最老的先）。如果 Dynamo 最终删除了调节对象状态所需的较早的向量时钟，Dynamo 将无法实现最终的一致性。Dynamo 的作者注意到这是一个潜在的问题，但没有说明如何解决这个问题。他们确实提到，这个问题还没有在他们的任何生产系统中出现。  


## 无冲突复制的数据类型（CRDTs）
处理冲突的一个更直接的方法是通过使用 CRDTs (Conflict-free replicated data types)。为了利用 CRDTs，需要对数据进行建模，使并发的变化可以以任何顺序应用于数据，并产生相同的最终结果。这样一来，系统就不需要担心任何排序保证。亚马逊的购物车是 CRDT 的一个很好的例子。当用户向购物车添加两件物品（A 和 B）时，添加 A 和 B 的这两个操作可以在任何节点上以任何顺序进行，因为最终的结果是这两件物品被添加到购物车中。(从购物车中删除被模拟为负添加)。任何两个收到相同更新的节点都会看到相同的最终结果，这个想法被称为强最终一致性。Riak 有一些内置的 CRDTs。  


## 最后写入胜利 (LWW)
不幸的是，将数据建模为 CRDTs 并不容易。在许多情况下，这需要许多的投入。因此，具有客户端分辨率的矢量时钟被认为足够好。  

除了矢量时钟，Dynamo 也提供了在服务器端自动解决冲突的方法。Dynamo（和 Apache Cassandra）经常使用一个简单的冲突解决策略：基于挂钟的时间戳的最后写入赢（LWW）。LWW 很容易导致数据丢失。例如，如果两个冲突的写入同时发生，就相当于抛硬币决定扔掉哪个写入。  


## Dynamo put() & get() 操作生命周期
### 选择协调节点的策略
Dynamo 客户端可以使用以下两种策略中的一种，为他们的 get() 和 put() 请求选择一个节点。  
* 客户端可以通过一个通用的负载均衡器路由他们的请求。
* 客户端可以使用一个分区感知的客户端库，将请求路由到延迟较低的适当协调器节点。
  
在第一种情况下，负载均衡器决定请求将被路由到哪里，而在第二种策略中，客户端选择连接的节点。这两种方法都有各自的好处。  

在第一种策略中，客户端不知道 Dynamo 环，这有助于扩展性，并使 Dynamo 的架构松散耦合。然而，在这种情况下，由于负载均衡器可以将请求转发给环中的任意节点，它所选择的节点有可能不在偏好列表中。这将导致一个额外的跳数，因为请求将被中间节点转发到偏好列表中的一个节点。  

第二种策略有助于实现更低的延迟，因为在这种情况下，客户端维护一个环形副本，并将请求转发到偏好列表中的适当节点。因为这个选项，Dynamo 也被称为零跳 DHT，因为客户端可以直接连接有目标数据的节点。然而，在这种情况下，Dynamo 对负载分配和请求处理没有太多的控制。  

![](./How%20Clients%20connect%20to%20Dynamo.png)  


## 一致性协议
Dynamo 使用一个类似于法定人数系统的一致性协议 (Quorum Systems)。如果 R/W 是必须分别参与成功的读/写操作的最小节点数。  
* 那么 R + W > N 就会产生一个类似于法定人数的系统
* 一个共同的（N, R, W）配置，Dynamo 使用的是 (3, 2, 2)。
  * (3, 3, 1): 快 W，慢 R，不持久化
  * (3, 1, 3): 快 R，慢 W，持久化
* 在这个模型中，get() 或 put() 操作的延迟取决于最慢的副本。由于这个原因，R 和 W 通常被配置为小于 N 以提供更好的延时。
* 一般来说，W 的低值和 R 的低值会增加不一致的风险，因为即使大多数副本没有处理，写请求也会被视为成功并返回给客户端。这也为持久性引入了一个脆弱的窗口，即使只在少数节点上被持久化，写请求仍被成功返回给客户端。
* 对于读和写操作，请求被转发到第一个 N 健康的节点。

### put() 过程
Dynamo 的 put() 请求将经历以下步骤：  
1. 协调器生成一个新的数据版本和矢量时钟组件。
2. 将新数据保存在本地。
3. 从偏好列表中向排名最高 N-1 的健康节点发送写请求。
4. 在收到 W-1 确认后，put() 操作可被认为是成功了。

### get() 过程
Dynamo 的 get() 请求将经历以下步骤：  
1. 协调器从偏好列表中的 N-1 个排名最高的健康节点请求数据版本。
2. 等待直到 R-1 回应。
3. 协调员通过一个矢量时钟处理因果数据版本。
4. 将所有相关的数据版本返回给调用者。

## 通过状态机处理请求
每个客户端请求都会在接收客户端请求的节点上创建一个状态机。状态机包含用于识别负责 Key 的节点、发送请求、等待响应、可能进行重试、处理响应以及为客户端打包响应的所有逻辑。每个状态机实例只处理一个客户端请求。例如，读取操作实现以下状态机：
1. 向节点发送读取请求。
2. 等待所需响应的最少数量。
3. 如果在给定的时间限制内收到的回复太少，则请求失败。
4. 否则，收集所有数据版本并确定要返回的数据版本。
5. 如果启用了版本控制，则执行语法协调并生成一个不透明的写入上下文，其中包含包含所有剩余版本的向量时钟。

在读取响应返回给调用者后，状态机会等待一小段时间以接收任何未完成的响应。如果在任何响应中返回过时版本，协调器将使用最新版本更新这些节点。此过程称为读取修复，因为它修复错过了最近更新的副本。  

如上所述，put() 请求由首选项列表中的前 N ​​个节点之一协调。尽管总是希望前 N 个节点中的第一个节点来协调写入，从而在一个位置序列化所有写入，但这种方法导致 Dynamo 的负载分布不均匀。这是因为请求负载不是在对象之间均匀分布的。为了解决这个问题，允许首选项列表中的前 N ​​个节点中的任何一个来协调写入。特别是，由于每个写操作通常都在读操作之后，因此写操作的协调器被选为对前一个读操作响应最快的节点，该节点存储在请求的上下文信息中。这种优化使 Dynamo 能够选择具有由先前读取操作读取的数据的节点，从而增加获得 "read-your-writes" 一致性的机会。  


## 通过默克尔树的反熵（Anti-entropy）
Dynamo 在处理读取请求时使用矢量时钟来消除冲突。如果一个副本明显落后于其他副本，则仅使用矢量时钟可能需要很长时间才能解决冲突。如果能够在后台自动解决一些冲突会更好，为此，需要快速比较位于不同副本上的一系列数据的两个副本，并准确找出哪些部分不同。  

### 什么是默克尔树？
副本可以包含大量数据。简单地分割数据范围并校验是不现实的：有太多的数据要传送。因此，Dynamo 使用 Merkle 树来比较范围的副本（replicas）。Merkle 树是哈希的二叉树，其中每个内部节点是其两个子节点的哈希，每个叶节点是原始数据的一部分的哈希。  
![](./Merkle%20Tree.png)  
Merkle 树上的数据比较逻辑其实很简单：  
1. 比较两棵树的根哈希。
2. 如果它们相等，则停止。
3. 递归左右孩子。

最终，这意味着副本准确地知道范围的哪些部分是不同的，并且交换的数据量被最小化。  

### Merkle 树的优缺点
使用 Merkle 树的主要优点是可以独立检查树的每个分支，而无需节点下载整个树或整个数据集。因此，Merkle 树最大限度地减少了需要传输以进行同步的数据量，并减少了在反熵过程中执行的磁盘读取次数。  

使用 Merkle 树的缺点是当节点加入或离开时，许多键范围会发生变化，因此需要重新计算树。  


## Gossip 协议
### 什么是 gossip 协议？
Dynamo 使用 gossip 协议，使每个节点能够跟踪集群中其他节点的状态信息，比如哪些节点是可访问的，它们负责哪些键范围等等（这基本上是哈希环的副本）。节点彼此共享状态信息以保持同步。Gossip 协议是一种点对点通信机制，其中节点定期交换关于自己和他们知道的其他节点的状态信息。每个节点每秒启动一次 gossip 轮次，以与另一个随机节点交换有关自身和其他节点的状态信息。这意味着任何新事件最终都会通过系统传播，并且所有节点都会快速了解集群中的所有其他节点。  
![](./Gossip%20Protocol.png)  


## 结论
以下是 Dynamo 的几个优点：
* 分布式：Dynamo 可以在大量机器上运行。
* 去中心化：Dynamo 是去中心化的；不需要任何中央协调员来监督业务。所有节点都是相同的，可以执行 Dynamo 的所有功能。
* 可扩展性：通过向集群添加更多节点，Dynamo 可以轻松进行水平扩展。无需人工干预或重新平衡。此外，Dynamo 在商用硬件上实现了线性可扩展性和经过验证的容错性。
* 高可用：Dynamo 具有容错性，即使一个或多个节点或数据中心出现故障，数据仍然可用。
* 容错可靠：由于数据被复制到多个节点，容错性非常高。
* 可调一致性：使用 Dynamo，应用程序可以调整数据可用性和一致性之间的权衡，通常通过配置复制因子和一致性级别设置。
* 耐用：Dynamo 永久存储数据。
* 最终一致：Dynamo 接受强一致性与高可用性之间的权衡。

以下列表包含对 Dynamo 设计的批评：  
* 每个 Dynamo 节点都包含整个 Dynamo 路由表。这可能会影响系统的可扩展性，因为随着向系统中添加节点，此路由表将变得越来越大。
* Dynamo 似乎暗示它力求对称，系统中的每个节点都有相同的角色和职责集，但后来它指定了一些节点作为种子。种子是外部可发现的特殊节点。这些用于帮助防止 Dynamo 环中的逻辑分区。这似乎违反了 Dynamo 的对称原理。
* 尽管安全性不是问题，因为 Dynamo 仅供内部使用，但 DHT 可能容易受到几种不同类型的攻击。虽然亚马逊可以假设一个受信任的环境，但有时有缺陷的软件可能会以非常类似于恶意行为者的方式行事。
* Dynamo 的设计可以被描述为 “泄漏抽象”，客户端应用程序经常被要求管理不一致性，并且用户体验不是 100% 无缝的。例如，购物车项目中的不一致可能会导致用户认为该网站有问题或不可靠。



# Dynamo: Amazon’s Highly Available Key-value Store
[Original Paper](./amazon-dynamo-sosp2007.pdf)  

## 重点
### 简介
* 系统的可靠性和可扩展性取决于其应用程序状态的管理方式。
  * Amazon 使用由数百个服务组成的高度分散、松散耦合、面向服务的架构。在这种环境中，特别需要始终可用的存储技术。
* Dynamo 用于管理有非常高的可靠性要求的服务或者需要可以严格控制可用性、一致性、成本效益、性能之间的权衡改变的服务。
  * 亚马逊的平台有一组非常多样化的应用程序，具有不同的存储要求。一组选定的应用程序需要一种足够灵活的存储技术，以使应用程序设计人员能够根据这些折衷适当地配置其数据存储，从而以最具成本效益的方式实现高可用性和保证性能。
* 某些服务只需要对数据存储进行主键访问，使用关系数据库的常见模式会导致效率低下并限制可扩展性和可用性。
* 使用 Dynamo，更新期间副本之间的一致性通过类似仲裁的技术和分散的副本同步协议（基于 gossip 的分布式故障检测和成员资格协议）来维护。
* 这项工作对研究界的主要贡献是评估如何组合不同的技术以提供单一的高可用性系统。

### 背景
* 传统上，生产系统将其状态存储在关系数据库中。然而，对于状态持久化的许多更常见的使用模式，关系数据库是一种远非理想的解决方案。这些服务中的大多数仅通过主键存储和检索数据，不需要 RDBMS 提供的复杂查询和管理功能。
#### System Assumptions and Requirements
* 查询模型：对由键唯一标识的数据项进行简单的读写操作。状态（State）存储为由唯一键标识的二进制对象（即 blob）。
  * Dynamo 针对需要存储相对较小（通常小于 1 MB）对象的应用程序。
* ACID：Dynamo 不提供任何隔离保证，只允许单键更新。
#### Service Level Agreements (SLA)
* 为了保证应用程序可以在有限的时间内交付其功能，平台中的每个依赖项都需要以更严格的边界交付其功能。客户和服务参与服务水平协议 (SLA)，这是一份正式协商的合同，其中客户端和服务端就几个与系统相关的特征达成一致。 
* 在亚马逊去中心化的面向服务的基础设施中，SLA 发挥着重要作用。例如，对电子商务站点之一的页面请求通常需要渲染引擎通过向 150 多个服务发送请求并组合构建它们的响应。这些服务通常有多个依赖关系，这些依赖关系通常是其他服务 ![](./Service-oriented%20architecture%20of%20Amazon's%20platform.png)  
#### Design Considerations
*商业系统中使用的数据复制算法传统上执行同步副本协调，以提供高度一致的数据访问接口。为了实现这种级别的一致性，这些算法被迫在某些故障场景下权衡数据的可用性。例如，不是处理答案正确性的不确定性，而是在绝对确定它是正确的之前使数据不可用。从非常早期的复制数据库工作中可以看出，在处理网络故障的可能性时，强一致性和高数据可用性不能同时实现。*
* 对于容易出现服务器和网络故障的系统，可以通过使用乐观复制技术来提高可用性，其中允许更改在后台传播到副本，并且可以容忍并发、断开连接的工作。
* 一个重要的设计考虑是决定何时执行解决更新冲突的过程，即是否应该在读取或写入期间解决冲突。许多传统数据存储在写入期间执行冲突解决，并保持读取复杂性简单。在此类系统中，如果数据存储在给定时间无法到达所有（或大部分）副本，则可能会拒绝写入。与之相对的，Dynamo 针对 "始终可写" 的数据存储（即高度可用于写入的数据存储）来设计。
* 下一个设计选择是谁执行冲突解决过程。这可以由数据存储或应用程序完成。如果冲突解决是由数据存储完成的，那么它的选择是相当有限的，在这种情况下，数据存储只能使用简单的策略（例如 "最后写入获胜"）来解决冲突更新。另一方面，由于应用程序知道数据模式，它可以决定最适合其客户体验的冲突解决方法。

设计中包含的其他关键原则是：
* 增量可扩展性：Dynamo 应该能够一次横向扩展一个存储主机（以下称为 "节点"），对系统运营商和系统本身的影响最小。
* 对称性：Dynamo 中的每个节点都应该拥有与其对等节点相同的职责集；不应该有一个或多个具有特殊角色或额外职责的节点。根据经验，对称性简化了系统配置和维护的过程。
* 去中心化：对称性的延伸，设计应该有利于去中心化的点对点技术而不是集中控制。过去，集中控制会导致中断，目标是尽可能避免这种情况。这导致了一个更简单、更可扩展和更可用的系统。
* 异质性：系统需要能够利用其运行的基础设施中的异质性。例如工作分配必须与各个服务器的能力成比例。这对于添加具有更高容量的新节点而不必一次升级所有主机至关重要。

### 相关工作
#### 点对点系统
有几个点对点 (P2P) 系统已经研究了数据存储和分发的问题。第一代 P2P 系统，如 Freenet 和 Gnutella，主要用作文件共享系统。这些是非结构化 P2P 网络的示例，其中对等点之间的覆盖链接是任意建立的。在这些网络中，搜索查询通常通过网络泛滥，以找到尽可能多的共享数据的对等方。P2P 系统发展到下一代，成为广为人知的结构化 P2P 网络。这些网络采用全球一致的协议来确保任何节点都可以有效地将搜索查询路由到具有所需数据的某个对等点。Pastry 和 Chord 等系统使用路由机制来确保可以在有限的跳数内回答查询。为了减少多跳路由引入的额外延迟，一些 P2P 系统采用 O(1) 路由，其中​​每个对等点在本地维护足够的路由信息​​，以便它可以将请求（访问数据项）路由到恒定数量内的适当对等节点。各种存储系统，例如 Oceanstore 和 PAST 都建立在这些路由覆盖之上。  
#### 分布式文件系统和数据库
*文件系统和数据库系统社区已经广泛研究了针对性能、可用​​性和持久性分发数据。与仅支持平面命名空间的 P2P 存储系统相比，分布式文件系统通常支持分层命名空间。Ficus 和 Coda 等系统以牺牲一致性为代价以实现文件复制高可用性。通常使用专门的冲突解决程序来管理更新冲突。Farsite 系统是一个分布式文件系统，不使用任何像 NFS 这样的中心化服务器。Farsite 使用复制实现了高可用性和可扩展性。Google 文件系统是另一个分布式文件系统，用于托管 Google 内部应用程序的状态。GFS 使用简单的设计，使用单个主服务器来托管整个元数据，并将数据分成块并存储在块服务器中。Bayou 是一个分布式关系数据库系统，它允许断开连接的操作并提供最终的数据一致性。*  
*这些系统允许断开连接的操作，并且对网络分区和中断等问题具有弹性，而且冲突解决程序不同：有的执行系统级冲突解决，有的允许应用程序级解决，所有这些都保证了最终的一致性。与这些系统类似，Dynamo 允许读取和写入操作在网络分区期间继续进行，并使用不同的冲突解决机制解决更新的冲突。像 FAB 这样的分布式块存储系统将大尺寸的对象拆分成更小的块，并以高可用的方式存储每个块。与这些系统相比，键值存储更适合这种情况，因为：（a）它旨在存储相对较小的对象（大小 < 1M）、（b）键值存储更容易在每个应用配置。Antiquity 是一种广域分布式存储系统，旨在处理多个服务器故障。它使用安全日志来保持数据完整性，在多台服务器上复制每个日志以实现持久性，并使用拜占庭容错协议来确保数据一致性。与 Antiquity 相比，Dynamo 并不关注数据完整性和安全性问题，而是为可信环境而构建的。Bigtable 是一个用于管理结构化数据的分布式存储系统。它维护一个稀疏的多维排序图，并允许应用程序使用多个属性访问其数据。与 Bigtable 相比，Dynamo 的目标是只需要键/值访问的应用程序，主要关注高可用性，即使在网络分区或服务器故障之后也不会拒绝更新。*  
![](./Partitioning%20and%20replication%20of%20keys%20in%20Dynamo%20ring.png)  
传统的关系数据库系统的复制关注的是保证复制数据的强一致性问题。尽管强一致性为应用程序编写者提供了一种方便的编程模型，但这些系统在可扩展性和可用性方面受到限制。这些系统无法处理网络分区，因为它们通常提供强大的一致性保证。
#### 讨论
使用 Dynamo 的应用程序不需要支持分层命名空间（许多文件系统中的规范）或复杂的关系模式（传统数据库支持）。  
Dynamo 专为延迟敏感的应用程序而构建，这些应用程序需要在几百毫秒内执行至少 99.9% 的读写操作。为了满足这些严格的延迟要求，必须避免通过多个节点路由请求（这是 Chord 和 Pastry 等几个 DHT/分布式哈希表系统采用的典型设计）。这是因为多跳路由增加了响应时间的可变性，从而增加了较高百分位数的延迟。Dynamo 可以被描述为零跳 DHT，其中每个节点在本地维护足够的路由信息​​以将请求直接路由到适当的节点。  

### 系统架构
需要在生产环境中运行的存储系统的架构很复杂。除了实际的数据持久化组件外，系统还需要具有可扩展且健壮的解决方案，用于负载平衡、成员资格和故障检测、故障恢复、副本同步、过载处理、状态转移、并发和作业调度、请求编组、请求路由、系统监控报警、配置管理。本文重点介绍 Dynamo 中使用的核心分布式系统技术：分区、复制、版本控制、成员资格、故障处理和扩展。  
![](./Summary%20of%20techniques%20used%20in%20Dynamo%20and%20their%20advantages.png)  
#### 系统接口
上下文对对于调用者不透明的对象的系统元数据进行编码，包括诸如对象版本之类的信息。上下文信息与对象一起存储，以便系统可以验证放置请求中提供的上下文对象的有效性。Dynamo 将调用者提供的键和对象都视为不透明的字节数组。它在键 Key 上应用 MD5 哈希以生成 128 位标识符，用于确定负责为 Key 提供服务的存储节点。  
#### 分区算法
Dynamo 的关键设计要求之一是它必须增量扩展。这需要一种机制来在系统中的一组节点（即存储主机）上动态划分数据。 Dynamo 的分区方案依赖于一致的散列（环）来将负载分布在多个存储主机上。  
#### 复制
为了实现高可用性和持久性，Dynamo 将其数据复制到多个主机上。每个数据项在 N 个主机上复制。每个键 Key 都分配给一个协调器节点。协调器负责复制其范围内的数据项。除了本地存储其范围内的每个键外，协调器还在环中的 N-1 个顺时针后继节点上复制这些键。这导致了一个系统，其中每个节点负责它与其第 N 个前任之间的环区域。  
负责存储特定密钥的节点列表称为首选项列表。  
#### 数据版本控制
与第一个教程类似（被覆盖的数据可被垃圾收集）。  
![](./Version%20Evolution%20of%20an%20Object%20Over%20Time.png)  
矢量时钟的一个可能问题是，如果许多服务器协调对对象的写入，矢量时钟的大小可能会增加。实际上，这不太可能，因为写入通常由首选项列表中的前 N ​​个节点之一处理。在网络分区或多个服务器故障的情况下，写入请求可能由不在首选项列表中的前 N ​​个节点中的节点处理，从而导致矢量时钟的大小增长。在这些情况下，希望限制矢量时钟的大小。为此，Dynamo 采用以下时钟截断方案：与每个（节点、计数器）对一起，Dynamo 存储一个时间戳，指示节点最后一次更新数据项的时间。当向量时钟中的（节点，计数器）对的数量达到阈值（比如 10）时，从时钟中删除最旧的对。显然，这种截断方案会导致协调效率低下，因为无法准确地导出后代关系。然而，这个问题并没有在生产中浮出水面，因此这个问题还没有得到彻底的调查。  
#### get() 和 put() 操作的执行
Dynamo 中的任何存储节点都有资格接收客户端获取和放置任何 Key 的操作。客户端可以使用两种策略来选择节点：（1）通过通用负载均衡器路由其请求，该负载均衡器将根据负载信息选择节点，或（2）使用分区感知客户端库直接路由请求到适当的协调节点（处理读取或写入操作的节点称为协调器节点）。第一种方法的优点是客户端不必在其应用程序中链接任何特定于 Dynamo 的代码，而第二种策略可以实现更低的延迟，因为它跳过了潜在的转发步骤。  
为了保持其副本之间的一致性，Dynamo 使用类似于仲裁系统中使用的一致性协议。该协议有两个关键的可配置值：R 和 W。R 是必须参与成功读取操作的最小节点数。 W 是必须参与成功写入操作的最小节点数。设置 R 和 W 使得 R + W > N 产生一个类 quorum 的系统。在此模型中，get（或 put）操作的延迟由最慢的 R（或 W）副本决定。因此，R 和 W 通常配置为小于 N，以提供更好的延迟。  
#### 失败处理：Hinted Handoff
与第一个教程类似。  
使用 Hinted Handoff，Dynamo 确保读写操作不会由于临时的节点或网络故障而失败。需要最高可用性级别的应用程序可以将 W 设置为 1，这确保只要系统中的单个节点已将 Key 持久地写入其本地存储，就视为写入成功。因此，只有当系统中的所有节点都不可用时，写入请求才会被拒绝。然而，在实践中，生产中的大多数亚马逊服务都设置了更高的 W 以满足所需的持久化级别。  
高可用的存储系统必须能够处理整个数据中心的故障。Dynamo 的配置使得每个对象都可以跨多个数据中心进行复制。本质上，Key 的偏好列表是这样构建的，使得存储节点分布在多个数据中心。  
#### 永久性故障处理：副本同步
如果系统成员流失率低且节点故障是短暂的，则 Hinted Handoff 效果最佳。在某些情况下，提示副本在返回到原始副本节点之前变得不可用。为了处理这个和其他对持久性的威胁，Dynamo 实现了一个反熵（副本同步）协议来保持副本同步。  
**为了更快地检测副本之间的不一致并最大限度地减少传输的数据量，Dynamo 使用了 Merkle 树。Merkle 树是一个哈希树，其中叶子是单个键值的哈希值。树中较高的父节点是它们各自子节点的哈希值。Merkle 树的主要优点是可以独立检查树的每个分支，而不需要节点下载整个树或整个数据集。此外，Merkle 树有助于减少需要传输的数据量，同时检查副本之间的不一致。例如，如果两棵树的根的哈希值相等，则树中的叶子节点的值相等，节点不需要同步。如果不是，则意味着某些副本的值不同。在这种情况下，节点可能会交换子节点的哈希值，并且该过程继续进行，直到它到达树的叶子，此时主机可以识别“不同步”的键。Merkle 树最大限度地减少了同步传输所需的数据量，并减少了在反熵过程中执行的磁盘读取次数。**  
Dynamo 使用 Merkle 树进行反熵，如下所示：每个节点为其托管的每个键范围（虚拟节点覆盖的键集）维护一个单独的 Merkle 树。这允许节点比较键范围内的键是否是最新的。在该方案中，两个节点交换与它们共同托管的键范围相对应的 Merkle 树的根。随后，使用上述树遍历方案，节点确定它们是否有任何差异并执行适当的同步操作。这种方案的缺点是，当节点加入或离开系统时，许多键范围会发生变化，从而需要重新计算树。然而，这个问题可以通过细化分区方案来解决。  
#### 成员关系和故障检测
* 环 Membership
  * 成员（Membership）更改形成历史记录，因为可以多次删除和添加节点。基于 gossip 的协议传播成员变化并保持成员的最终一致视图。每个节点每秒都会联系一个随机选择的对等节点，这两个节点有效地协调它们持久的成员更改历史。
  * 当一个节点第一次启动时，它会选择它的 Token 集（一致哈希空间中的虚拟节点）并将节点映射到它们各自的 Token 集。映射保存在磁盘上，最初只包含本地节点和 Token 集。存储在不同 Dynamo 节点的映射在协调成员更改历史的同一通信交换期间协调。因此，分区和放置信息也通过基于 gossip 的协议传播，并且每个存储节点都知道其对等点处理的 Token 范围。
* 外部发现
  * 为了防止逻辑分区，一些 Dynamo 节点扮演种子的角色。种子是通过外部机制发现并为所有节点所知的节点。因为所有节点最终都会通过种子来协调它们的成员资格，所以逻辑分区的可能性很小。种子可以从静态配置或配置服务中获得。通常，种子是 Dynamo 环中功能齐全的节点。
#### 故障检测
Dynamo 中的故障检测用于避免在 get() 和 put() 操作期间以及在传输分区和提示副本时尝试与无法访问的对等方进行通信。为了避免失败的通信尝试，纯本地的故障检测概念就足够了。分散式故障检测协议使用简单的 gossip-style 协议，使系统中的每个节点都能够了解其他节点的上线与宕机。  
#### 添加/删除存储节点
当一个新节点（比如 X）被添加到系统中时，它会被分配一些随机分散在环上的 Token。  

### 实现
在 Dynamo 中，每个存储节点都具有三个主要的软件组件：请求协调、成员资格和故障检测以及本地持久性引擎。  
Dynamo 的本地持久性组件允许插入不同的存储引擎。正在使用的引擎有如 MySQL 和具有持久性后备存储的内存缓冲区。设计可插拔持久性组件的主要原因是选择最适合应用程序访问模式的存储引擎。应用程序根据其对象大小分布选择 Dynamo 的本地持久性引擎。  
请求协调组件建立在事件驱动的消息传递基板之上，其中消息处理管道被分成多个阶段，类似于 SEDA 架构。每个客户端请求都会导致在接收客户端请求的节点上创建一个状态机（具体与第一个教程类似）。  

### 经验教训
实践与配置（与第一个教程类似）。  
如果 W 设置为 1，那么只要系统中至少有一个节点可以成功处理写入请求，系统就永远不会拒绝写入请求。但是，W 和 R 的低值会增加不一致的风险，因为写入请求被视为成功并返回给客户端，即使它们没有被大多数副本处理。当写入请求成功返回到客户端时，即使它仅在少数节点上持久化，这也会引入持久性漏洞窗口。  
#### Balancing Performance and Durability
#### 确保均匀的负载分布
ToDo...  
* Strategy 1: T random tokens per node and partition by token value
* Strategy 2: T random tokens per node and equal sized partitions
* Strategy 3: Q/S tokens per node, equal-sized partitions
* ToDo...  
#### 不同版本：何时和多少？
ToDo...（与第一个教程类似）
#### 客户端驱动或服务器驱动协调
ToDo...（与第一个教程类似）
#### 平衡后台与前台任务
除了正常的前台 put/get 操作外，每个节点还执行不同类型的后台任务，用于副本同步和数据切换（由于 hinted 或添加/删除节点），而且有必要确保后台任务仅在常规关键操作未受到显着影响时运行。为此，后台任务需要与准入控制机制相结合。每个后台任务都使用这个控制器来保留资源（例如数据库）的运行时切片，在所有后台任务之间共享。采用基于监控的前台任务性能的反馈机制来更改后台任务可用的切片数量。准入控制器在执行“前台”放置/获取操作时不断监视资源访问的行为。监控的方面包括磁盘操作的延迟、由于锁争用和事务超时导致的数据库访问失败以及请求队列等待时间。
#### 讨论
对于想要使用 Dynamo 的新应用程序，需要在开发的初始阶段进行一些分析，以选择适合业务案例的正确冲突解决机制。  
Dynamo 采用完全会员模型，其中每个节点都知道其对等节点托管的数据。为此，每个节点都主动与系统中的其他节点 gossips（闲聊）完整的路由表。该模型适用于包含数百个节点的系统。然而，将这样的设计扩展到运行数万个节点并非易事，因为维护路由表的开销随着系统大小的增加而增加。这个限制可以通过向 Dynamo 引入分层扩展来克服。不过，O(1) DHT 系统可积极解决这个问题。  



# Others
* [Distributed system | 从 Amazon Dynamo 看分布式系统](https://www.youtube.com/watch?v=2UkujoUul4w)
* [Cassandra | Dynamo 的一个实现案例](./../../../Common%20Data%20Structure%20and%20Data%20Type/Data%20Structure%20Implementation/LSMTree/README.md#Cassandra)
