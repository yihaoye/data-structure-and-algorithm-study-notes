目标#
设计一个高度可用（即可靠）、高度可扩展、完全去中心化的分布式键值存储。

什么是Dynamo？
Dynamo是亚马逊为其内部使用而开发的一种高可用键值存储。许多亚马逊服务，如购物车、畅销书列表、销售排名、产品目录等，只需要对数据进行主键访问。一个多表的关系型数据库系统对于这类服务来说是一种过度，同时也会限制可扩展性和可用性。Dynamo提供了一个灵活的设计，让应用程序选择他们想要的可用性和一致性水平。

背景#
Dynamo--不要与DynamoDB混淆，后者的设计灵感来自于Dynamo--是一个分布式键值存储系统，在大规模的情况下提供 "永远在线"（或高度可用）的体验。在CAP定理的术语中，Dynamo属于AP系统的范畴（即可用和分区容忍），并且是以牺牲强一致性为代价来设计高可用性和分区容忍。将Dynamo设计成一个高可用系统的主要动机是观察到系统的可用性与所服务的客户数量直接相关。因此，主要目标是系统即使不完善，也应该对客户可用，因为它能带来更多的客户满意度。另一方面，不一致的问题可以在后台解决，而且大多数时候客户不会注意到它们。从这个核心原则出发，Dynamo积极地对可用性进行了优化。

Dynamo的设计具有很大的影响力，因为它启发了许多NoSQL数据库，如Cassandra、Riak和Voldemort--更不用说亚马逊自己的DynamoDB。

设计目标#
如上所述，Dynamo的主要目标是高度可用。下面是其其他设计目标的总结。

可扩展性。该系统应该是高度可扩展的。我们应该能够把一台机器扔进系统中，看到相应的改进。
分散的。为了避免单点故障和性能瓶颈，不应该有任何中央/领导的过程。
最终一致的。数据可以被乐观地复制，成为最终一致的。这意味着，与其为确保整个系统的数据正确性而产生写入时间成本（即强一致性），不如在其他时间（例如，在读取过程中）解决不一致的问题。最终的一致性被用来实现高可用性。

![](./Dynamo%20High%20Level.png)  
分布式键值存储的高层次视图
Dynamo的使用案例#
默认情况下，Dynamo是一个最终一致的数据库。因此，任何不关心强一致性的应用都可以利用Dynamo。尽管Dynamo可以支持强一致性，但它会对性能产生影响。因此，如果强一致性是一个应用程序的要求，那么Dynamo可能不是一个好的选择。

Dynamo在亚马逊被用来管理那些对可靠性要求非常高的服务，并且需要严格控制可用性、一致性、成本效益和性能之间的权衡。亚马逊的平台有非常多样化的应用，有不同的存储要求。许多应用选择Dynamo是因为它可以灵活地选择适当的权衡，以最经济的方式实现高可用性和保证性能。

亚马逊平台上的许多服务只要求对数据存储的主键访问。对于这样的服务，使用关系型数据库的常见模式会导致效率低下，并限制可扩展性和可用性。Dynamo提供了一个简单的只有主键的接口，以满足这些应用程序的要求。

系统应用程序接口#
Dynamo客户端使用put()和get()操作来写入和读取对应于指定键的数据。这个键唯一地标识了一个对象。

get(key)。get操作找到与给定键相关的对象所在的节点，并返回一个单一的对象或一个具有冲突版本的对象列表以及一个上下文。上下文包含了关于对象的编码元数据，这些元数据对调用者没有意义，包括对象的版本等信息（下面会有更多的介绍）。

put(key, context, object)。put操作找到与给定键相关的对象应该被存储的节点，并将给定的对象写到磁盘上。上下文是一个值，通过get操作返回，然后通过put操作送回。上下文总是与对象一起存储，并像一个cookie一样用来验证在投放请求中提供的对象的有效性。

Dynamo将对象和键都视为一个任意的字节数组（通常小于1MB）。它在键上应用MD5散列算法，生成一个128位的标识符，用来确定负责提供键的存储节点。



在高层次上，Dynamo是一个分布式哈希表（DHT），它在集群中进行复制，以实现高可用性和容错。

介绍。Dynamo的架构#
Dynamo的架构可以概括为以下几点（我们将在下面的课程中详细讨论所有这些概念）。

数据分布#
Dynamo使用一致哈希法在节点间分配数据。一致性散列也使得从Dynamo集群中添加或删除节点变得容易。

数据复制和一致性#
数据是乐观地复制的，即Dynamo提供最终的一致性。

处理临时故障#
为了处理临时故障，Dynamo将数据复制到系统中其他节点的模糊法定人数，而不是严格的多数法定人数。

节点间通信和故障检测#
Dynamo的节点使用gossip协议来跟踪集群状态。

高可用性#
Dynamo通过使用暗示交接使系统 "永远可写"（或高可用性）。

冲突解决和处理永久性故障#
由于没有写入时间保证节点在数值上达成一致，Dynamo使用其他机制来解决潜在冲突。

使用矢量时钟来跟踪值的历史，并在读取时调和不同的历史。
在后台，dynamo使用像Merkle树这样的反熵机制来处理永久性故障。
让我们逐一讨论这些概念。



什么是数据分区？
将数据分布在一组节点上的行为被称为数据分区。当我们试图分配数据时，有两个挑战。
1. 我们如何知道某个特定的数据将被存储在哪个节点上？
2. 当我们添加或删除节点时，我们如何知道哪些数据将从现有的节点转移到新的节点上？此外，当节点加入或离开时，我们怎样才能将数据移动降到最低？

一个天真的方法将是使用一个合适的哈希函数，将数据键映射到一个数字。然后，通过对这个数字和服务器的总数应用模数来找到服务器。比如说。

![](./Data%20partitioning%20through%20simple%20hashing.png)  
通过简单的散列法进行数据分区

上图中描述的方案解决了寻找存储/检索数据的服务器的问题。但是，当我们增加或删除一个服务器时，我们必须重新映射所有的键，并根据新的服务器数量移动数据，这将是一个完全混乱的过程

Dynamo使用一致性散列来解决这些问题。一致性散列算法帮助Dynamo将行映射到物理节点上，也确保在增加或删除服务器时，只有一小部分键会移动。

一致性散列：Dynamo的数据分布＃。
一致性散列将集群管理的数据表示为一个环。环中的每个节点都被分配一个数据范围。Dynamo使用一致散列算法来确定什么行被存储到什么节点。下面是一致散列环的一个例子。

![](./Consistent%20Hashing%20ring.png)  
一致性散列环

通过一致散列，环被划分为更小的预定义范围。每个节点被分配到这些范围中的一个。在Dynamo的术语中，范围的开始被称为token。这意味着每个节点将被分配一个令牌。分配给每个节点的范围是按以下方式计算的。

范围开始。 代币值
范围结束。   下一个令牌值-1

下面是上图中描述的四个节点的令牌和数据范围。
![](./Servers%20and%20token.png)  


每当Dynamo为put()或get()请求提供服务时，它执行的第一个步骤是对密钥应用MD5散列算法。这个散列算法的输出决定了数据位于哪个范围内，因此，数据将被存储在哪个节点上。正如我们在上面看到的，Dynamo的每个节点都应该存储一个固定范围的数据。因此，从数据键生成的哈希值告诉我们数据将被存储在哪个节点上。下面是一个例子，显示了数据如何在一致哈希环上分布。

![](./Distributing%20data%20on%20the%20consistent%20hashing%20ring.png)  
在一致散列环上分布数据

当一个节点被添加或从环中移除时，上面描述的一致散列方案非常有效；因为在这些情况下，只有下一个节点会受到影响。例如，当一个节点被移除时，下一个节点就会对存储在出站节点上的所有密钥负责。然而，这种方案会导致数据和负载分布不均匀。Dynamo在虚拟节点的帮助下解决了这些问题。

虚拟节点#
在任何分布式系统中添加和删除节点是很常见的。现有的节点可能会死亡，可能需要退役。同样地，新的节点可能会被添加到现有的集群中，以满足不断增长的需求。Dynamo通过使用虚拟节点（或Vnodes）有效地处理这些情况。

正如我们在上面看到的，基本的一致哈希算法为每个物理节点分配了一个令牌（或一个连续的哈希范围）。这是一种静态的范围划分，需要根据给定的节点数量来计算令牌。这种方案使得添加或替换一个节点成为一个昂贵的操作，因为在这种情况下，我们希望重新平衡并将数据分配给所有其他节点，导致移动大量数据。下面是一些与手动和固定划分范围相关的潜在问题。

* 增加或删除节点。增加或删除节点将导致重新计算令牌，对一个大型集群来说，会造成很大的管理开销。
* 热点。由于每个节点被分配一个大范围，如果数据分布不均匀，一些节点会成为热点。
* 节点重建。由于每个节点的数据被复制在固定数量的节点上（后面会讨论），当我们需要重建一个节点时，只有它的复制节点可以提供数据。这给复制节点带来了很大的压力，并可能导致服务下降。

为了处理这些问题，Dynamo引入了一种新的方案，将令牌分配给物理节点。与其给一个节点分配一个令牌，不如将哈希范围划分为多个较小的范围，每个物理节点被分配到这些较小范围中的多个。这些子范围中的每一个都被称为一个Vnode。有了Vnodes，而不是一个节点只负责一个令牌，而是负责许多令牌（或子范围）。

![](./Comparing%20Consistent%20Hashing%20ring%20with%20and%20without%20Vnodes.png)  
比较有Vnodes和无Vnodes的一致性哈希环

实际上，Vnodes是随机分布在集群中的，通常是不连续的，所以没有两个相邻的Vnodes被分配到同一个物理节点。此外，节点确实携带着其他节点的副本以实现容错。另外，由于集群中可能存在异质机器，一些服务器可能比其他服务器拥有更多的Vnodes。下图显示了物理节点A、B、C、D、E是如何使用一致哈希环的Vnodes的。每个物理节点被分配一组Vnodes，每个Vnode被复制一次。

![](./Mapping%20Vnodes%20to%20physical%20nodes%20on%20a%20Consistent%20Hashing%20ring.png)  
将Vnodes映射到一致哈希环的物理节点上

Vnodes#的优势
Vnodes有以下优点。

1. Vnodes通过将哈希范围划分为较小的子范围，帮助将负载更均匀地分布在集群上的物理节点上。这加快了添加或删除节点后的再平衡过程。当一个新的节点被添加时，它从现有的节点中接收许多Vnodes以保持集群的平衡。同样，当一个节点需要重建时，不是从固定数量的副本中获取数据，而是许多节点参与重建过程。
2. Vnodes使得维护一个包含异构机器的集群变得更加容易。这意味着，有了Vnodes，我们可以给强大的服务器分配较多的范围，给不太强大的服务器分配较少的范围。
3. 由于Vnodes有助于为每个物理节点分配较小的范围，因此出现热点的概率比基本的一致哈希方案要小得多，因为该方案每个节点使用一个大范围。




