# 机器学习 | Machine Learning

## [Intermediate Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)
### 模型
### pandas 读取、统计数据
```python
import pandas as pd

file_path = '../input/xxx-snapshot/data.csv'
data = pd.read_csv(file_path) 
data.describe()
```
### 决策树
```python
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(random_state=1)
model.fit(X, y) # X, y from above pandas data
```
### 模型验证
```python
from sklearn.metrics import mean_absolute_error

predicted = model.predict(X)
mean_absolute_error(y, predicted)
```
### 过拟合与欠拟合
### 随机森林
随机森林即多个决策树，主要用于提高决策树的实用性  
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, preds))
```

特点：
* 独立训练：随机森林中的每棵决策树是独立训练的。每棵树在训练时使用了一个从训练数据集中随机抽样得到的子集（有放回抽样，即bootstrap抽样）。
* 特征选择：在每个节点分裂时，随机森林从所有特征中随机选择一个子集来寻找最佳分裂点，这增加了树之间的差异性。
* 最终预测：对于分类问题，随机森林通过对所有树的预测结果进行投票来决定最终分类结果；对于回归问题，则通过对所有树的预测结果取平均值来得到最终预测结果。

### GBDT
即决策树集合，每棵决策树都是为了拟合上一个决策树的残差。  
通过迭代训练构造出一组弱的学习器/决策树，并把它们的结果加权求和作为最终的预测输出，其过程通常是：
1. 加载训练过程中生成的所有决策树及其权重
2. 从初始模型开始且通常是一个常数
3. 依次通过每棵决策树，根据输入数据进行预测，将每棵树的预测结果乘以对应的学习率，加到初始模型上，逐步累加得到最终预测结果

#### XGBoost
是 GBDT 的一种高效实现，进行了正则化以防止过拟合、学习率缩放控制单决策树影响以增强泛化能力、并行化、列抽样以增强多样性、缓存优化等多种优化。  

## [Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning)
### Single Neuron
`y = w * x + b`  

```python
from tensorflow import keras
from tensorflow.keras import layers

# Create a network with 1 linear unit
model = keras.Sequential([
    layers.Dense(units=1, input_shape=[3])
])
```
### Deep Neural Networks
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # the hidden ReLU layers
    layers.Dense(units=4, activation='relu', input_shape=[2]),
    layers.Dense(units=3, activation='relu'),
    # the linear output layer 
    layers.Dense(units=1),
])
```
### Stochastic Gradient Descent
随机梯度下降  
```python
model.compile(
    optimizer='adam', # The Optimizer - Stochastic Gradient Descent
    loss='mae',
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=10,
)
```
### 过拟合与欠拟合
```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=500,
    callbacks=[early_stopping], # put your callbacks in a list
    verbose=0,  # turn off training log
)
```
### Dropout and Batch Normalization
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(1024, activation='relu', input_shape=[11]),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1),
])
```
### Binary Classification
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(4, activation='relu', input_shape=[33]),
    layers.Dense(4, activation='relu'),    
    layers.Dense(1, activation='sigmoid'),
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy'], # 二元分类
)
```

## [Feature Engineering](https://www.kaggle.com/learn/feature-engineering)
// ToDo...  

## 自然语言处理
NLP
* 基础
  * 词嵌入 - 将词表示为连续向量的技术，能够捕捉词与词之间的语义关系。[Word2Vec](https://zh.wikipedia.org/wiki/Word2vec) 和 GloVe 是其中最常见的方法，其本质都是浅层神经网络模型
  * 词袋模型 - 文本表示方法，忽略了词的顺序，仅仅关注词的出现频率。每个文档被表示为一个词频向量。改进型 TF-IDF 还考虑词在文档集合中的普遍性，给予常见词较低权重，稀有词较高权重
* 中阶
  * 神经网络 RNNs 和 LSTM - 处理序列数据，能够捕捉文本中的上下文信息
* 进阶
  * 大语言模型

## 多模态学习
多模态学习（Multimodal Learning）：指的是利用多种不同类型的数据模态（例如文本、图像、音频等）来进行学习和决策的方法。它的目标是通过融合和联合建模多种数据模态，来提升机器学习系统的表现。多模态学习通常涉及到深度学习模型的构建，以便有效地处理和利用多种数据源的信息。  
本身并不是深度学习的特定方法或算法，而是一种可以在深度学习框架下应用的策略或方法论。它利用深度学习模型的能力来处理和融合多种类型的数据模态，从而提高系统在多模态环境中的表现。  
具体来说，多模态学习可以使用深度神经网络来构建和训练模型，例如使用卷积神经网络（CNN）、循环神经网络（RNN）、Transformer 等深度学习架构。这些模型可以针对不同的数据模态进行特定的表示学习和融合，以实现对多模态数据的联合建模和处理。  

## 机器学习中最优化问题的 P/NP 属性
在机器学习和深度学习领域，参数优化通常是一个关键的问题，而很多模型训练的目标是要最小化或最大化一个目标函数。这个优化问题可以被认为是一个最优化问题。  
一些机器学习问题的目标函数是凸的，这意味着可以使用梯度下降等基于梯度的优化方法来找到全局最优解，这通常是在多项式时间内可行的。  
然而，对于某些复杂的机器学习问题，目标函数可能是非凸的，这可能导致问题变得更困难，因为在非凸函数上找到全局最优解通常是 NP 问题。在这种情况下，通常使用梯度下降的变种，如随机梯度下降（SGD）或其他近似算法来找到次优解。  
by ChatGPT  
