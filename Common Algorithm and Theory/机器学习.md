# 机器学习 | Machine Learning

## [Intermediate Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)
### 模型
### pandas 读取、统计数据
```python
import pandas as pd

file_path = '../input/xxx-snapshot/data.csv'
data = pd.read_csv(file_path) 
data.describe()
```
### 决策树
```python
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(random_state=1)
model.fit(X, y) # X, y from above pandas data
```
### 模型验证
```python
from sklearn.metrics import mean_absolute_error

predicted = model.predict(X)
mean_absolute_error(y, predicted)
```
### 过拟合与欠拟合
### 随机森林
随机森林即多个决策树，主要用于提高决策树的实用性  
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, preds))
```

## [Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning)
### Single Neuron
`y = w * x + b`  

```python
from tensorflow import keras
from tensorflow.keras import layers

# Create a network with 1 linear unit
model = keras.Sequential([
    layers.Dense(units=1, input_shape=[3])
])
```
### Deep Neural Networks
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # the hidden ReLU layers
    layers.Dense(units=4, activation='relu', input_shape=[2]),
    layers.Dense(units=3, activation='relu'),
    # the linear output layer 
    layers.Dense(units=1),
])
```
### Stochastic Gradient Descent
随机梯度下降  
```python
model.compile(
    optimizer='adam', # The Optimizer - Stochastic Gradient Descent
    loss='mae',
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=10,
)
```
### 过拟合与欠拟合
```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=500,
    callbacks=[early_stopping], # put your callbacks in a list
    verbose=0,  # turn off training log
)
```
### Dropout and Batch Normalization
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(1024, activation='relu', input_shape=[11]),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1),
])
```
### Binary Classification
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(4, activation='relu', input_shape=[33]),
    layers.Dense(4, activation='relu'),    
    layers.Dense(1, activation='sigmoid'),
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy'], # 二元分类
)
```

## [Feature Engineering](https://www.kaggle.com/learn/feature-engineering)
// ToDo...  

## 机器学习中最优化问题的 P/NP 属性
在机器学习和深度学习领域，参数优化通常是一个关键的问题，而很多模型训练的目标是要最小化或最大化一个目标函数。这个优化问题可以被认为是一个最优化问题。  
一些机器学习问题的目标函数是凸的，这意味着可以使用梯度下降等基于梯度的优化方法来找到全局最优解，这通常是在多项式时间内可行的。  
然而，对于某些复杂的机器学习问题，目标函数可能是非凸的，这可能导致问题变得更困难，因为在非凸函数上找到全局最优解通常是 NP 问题。在这种情况下，通常使用梯度下降的变种，如随机梯度下降（SGD）或其他近似算法来找到次优解。  
by ChatGPT  
