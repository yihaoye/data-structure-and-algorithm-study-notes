# 机器学习 | Machine Learning

## [Intermediate Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)
### 模型
### pandas 读取、统计数据
```python
import pandas as pd

file_path = '../input/xxx-snapshot/data.csv'
data = pd.read_csv(file_path) 
data.describe()
```
### 决策树
```python
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(random_state=1)
model.fit(X, y) # X, y from above pandas data
```
### 模型验证
```python
from sklearn.metrics import mean_absolute_error

predicted = model.predict(X)
mean_absolute_error(y, predicted)
```
### 过拟合与欠拟合
### 随机森林
随机森林即多个决策树，主要用于提高决策树的实用性  
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, preds))
```

## [Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning)
### Single Neuron
`y = w * x + b`  

```python
from tensorflow import keras
from tensorflow.keras import layers

# Create a network with 1 linear unit
model = keras.Sequential([
    layers.Dense(units=1, input_shape=[3])
])
```
### Deep Neural Networks
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # the hidden ReLU layers
    layers.Dense(units=4, activation='relu', input_shape=[2]),
    layers.Dense(units=3, activation='relu'),
    # the linear output layer 
    layers.Dense(units=1),
])
```
### Stochastic Gradient Descent
随机梯度下降  
```python
model.compile(
    optimizer='adam', # The Optimizer - Stochastic Gradient Descent
    loss='mae',
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=10,
)
```
### 过拟合与欠拟合
```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=500,
    callbacks=[early_stopping], # put your callbacks in a list
    verbose=0,  # turn off training log
)
```
### Dropout and Batch Normalization
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(1024, activation='relu', input_shape=[11]),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1),
])
```
### Binary Classification
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(4, activation='relu', input_shape=[33]),
    layers.Dense(4, activation='relu'),    
    layers.Dense(1, activation='sigmoid'),
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy'], # 二元分类
)
```

## [Feature Engineering](https://www.kaggle.com/learn/feature-engineering)
// ToDo...  
